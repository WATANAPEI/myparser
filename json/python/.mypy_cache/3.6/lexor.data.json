{".class": "MypyFile", "_fullname": "lexor", "is_partial_stub_package": false, "is_stub": false, "names": {".class": "SymbolTable", "__doc__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "lexor.__doc__", "name": "__doc__", "type": "builtins.str"}}, "__file__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "lexor.__file__", "name": "__file__", "type": "builtins.str"}}, "__name__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "lexor.__name__", "name": "__name__", "type": "builtins.str"}}, "__package__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "lexor.__package__", "name": "__package__", "type": "builtins.str"}}, "contents": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": [], "fullname": "lexor.contents", "name": "contents", "type": "builtins.str"}}, "f": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": [], "fullname": "lexor.f", "name": "f", "type": "typing.TextIO"}}, "i": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": [], "fullname": "lexor.i", "name": "i", "type": "builtins.int"}}, "l": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": [], "fullname": "lexor.l", "name": "l", "type": {".class": "AnyType", "missing_import_name": null, "source_any": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, "type_of_any": 7}}}, "list": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": [], "fullname": "lexor.list", "name": "list", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "tokenize_orig": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "arg_kinds": [0], "arg_names": ["contents"], "flags": [], "fullname": "lexor.tokenize_orig", "name": "tokenize_orig", "type": null}}, "tokenizer": {".class": "SymbolTableNode", "cross_ref": "tokenizer", "kind": "Gdef"}}, "path": "/home/baystars/src/myparser/json/python/lexor.py"}